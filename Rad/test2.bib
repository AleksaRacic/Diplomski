
@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Downloads}},
	url = {https://www.zotero.org/download/},
	urldate = {2022-10-12},
	file = {Zotero | Downloads:/Users/racicaleksa/Zotero/storage/AXW7M5HT/download.html:text/html},
}

@misc{noauthor_zotero_nodate-1,
	title = {Zotero {\textbar} {Password} reset},
	url = {https://www.zotero.org/user/resetpassword/i5e26kr4k99hxkhxxwdlcdsqmnxkqn9n3d2lgl7e},
	urldate = {2022-10-12},
	file = {Zotero | Password reset:/Users/racicaleksa/Zotero/storage/GWQDID4Z/i5e26kr4k99hxkhxxwdlcdsqmnxkqn9n3d2lgl7e.html:text/html},
}

@misc{noauthor_zotero_nodate-2,
	title = {Zotero {\textbar} {Connectors}},
	url = {https://www.zotero.org/download/connectors},
	urldate = {2022-10-12},
	keywords = {d, s},
	file = {Zotero | Connectors:/Users/racicaleksa/Zotero/storage/WC5AKR4Q/connectors.html:text/html},
}

@misc{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	doi = {10.48550/arXiv.1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv:1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, GAN},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/Y9X7QXY9/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/A5VD6J4N/1701.html:text/html},
}

@misc{arjovsky_wasserstein_2017-1,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	language = {en},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv:1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Arjovsky et al. - 2017 - Wasserstein GAN.pdf:/Users/racicaleksa/Zotero/storage/Q3WL67QB/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf},
}

@article{reck_experimental_1994,
	title = {Experimental realization of any discrete unitary operator},
	volume = {73},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.73.58},
	doi = {10.1103/PhysRevLett.73.58},
	language = {en},
	number = {1},
	urldate = {2019-03-30},
	journal = {Physical Review Letters},
	author = {Reck, Michael and Zeilinger, Anton and Bernstein, Herbert J. and Bertani, Philip},
	month = jul,
	year = {1994},
	pages = {58--61},
}

@article{reck_experimental_1994-1,
	title = {Experimental realization of any discrete unitary operator},
	volume = {73},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.73.58},
	doi = {10.1103/PhysRevLett.73.58},
	language = {en},
	number = {1},
	urldate = {2019-03-30},
	journal = {Physical Review Letters},
	author = {Reck, Michael and Zeilinger, Anton and Bernstein, Herbert J. and Bertani, Philip},
	month = jul,
	year = {1994},
	pages = {58--61},
}

@misc{e3s_center_deep_nodate,
	title = {Deep {Learning} with {Coherent} {Nanophotonic} {Circuits}},
	url = {https://www.youtube.com/watch?v=Zu_2zz41Yno},
	urldate = {2019-03-30},
	author = {{E3S Center}},
}

@article{tait_neuromorphic_2017,
	title = {Neuromorphic photonic networks using silicon photonic weight banks},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-07754-z},
	doi = {10.1038/s41598-017-07754-z},
	language = {en},
	number = {1},
	urldate = {2019-03-30},
	journal = {Scientific Reports},
	author = {Tait, Alexander N. and de Lima, Thomas Ferreira and Zhou, Ellen and Wu, Allie X. and Nahmias, Mitchell A. and Shastri, Bhavin J. and Prucnal, Paul R.},
	month = dec,
	year = {2017},
	file = {Tait et al. - 2017 - Neuromorphic photonic networks using silicon photo.pdf:/Users/racicaleksa/Zotero/storage/RTHGBP2X/Tait et al. - 2017 - Neuromorphic photonic networks using silicon photo.pdf:application/pdf},
}

@misc{mos_optical_1999,
	title = {Optical neural network based on laser diode longitudinal modes},
	url = {http://repository.tue.nl/525477},
	language = {nl},
	urldate = {2019-03-30},
	publisher = {Technische Universiteit Eindhoven},
	author = {Mos, EC Evert},
	year = {1999},
	doi = {10.6100/ir525477},
	file = {Mos - 1999 - Optical neural network based on laser diode longit.pdf:/Users/racicaleksa/Zotero/storage/H2QI2RVC/Mos - 1999 - Optical neural network based on laser diode longit.pdf:application/pdf},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {On}-chip optical matrix-vector multiplier},
	url = {https://www.researchgate.net/publication/260868239_On-chip_optical_matrix-vector_multiplier},
	abstract = {PDF {\textbar} Matrix-vector multiplication is a fundamental operation in modern digital signal processing. Inspired by the intrinsic spatial parallelism of optics, many efforts have been made to develop optical apparatuses that can perform such a parallelizable operation. Here we report...},
	language = {en},
	urldate = {2019-03-30},
	journal = {ResearchGate},
	doi = {http://dx.doi.org/10.1117/12.2028585},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/CLQZ4URX/260868239_On-chip_optical_matrix-vector_multiplier.html:text/html},
}

@article{shen_deep_2017,
	title = {Deep {Learning} with {Coherent} {Nanophotonic} {Circuits}},
	volume = {11},
	issn = {1749-4885, 1749-4893},
	url = {http://arxiv.org/abs/1610.02365},
	doi = {10.1038/nphoton.2017.93},
	abstract = {Artificial Neural Networks are computational network models inspired by signal processing in the brain. These models have dramatically improved the performance of many learning tasks, including speech and object recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made to develop electronic architectures tuned to implement artificial neural networks that improve upon both computational speed and energy efficiency. Here, we propose a new architecture for a fully-optical neural network that, using unique advantages of optics, promises a computational speed enhancement of at least two orders of magnitude over the state-of-the-art and three orders of magnitude in power efficiency for conventional learning tasks. We experimentally demonstrate essential parts of our architecture using a programmable nanophotonic processor.},
	number = {7},
	urldate = {2019-03-30},
	journal = {Nature Photonics},
	author = {Shen, Yichen and Harris, Nicholas C. and Skirlo, Scott and Prabhu, Mihika and Baehr-Jones, Tom and Hochberg, Michael and Sun, Xin and Zhao, Shijie and Larochelle, Hugo and Englund, Dirk and Soljacic, Marin},
	month = jul,
	year = {2017},
	note = {arXiv: 1610.02365},
	keywords = {Fotonika, Physics - Computational Physics, Physics - Optics},
	pages = {441--446},
	file = {arXiv\:1610.02365 PDF:/Users/racicaleksa/Zotero/storage/GJW3FMQU/Shen et al. - 2017 - Deep Learning with Coherent Nanophotonic Circuits.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/Y77XTXBF/1610.html:text/html},
}

@misc{arjovsky_wasserstein_2017-2,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	doi = {10.48550/arXiv.1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv:1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/CJSDGKZI/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/H57HT67L/1701.html:text/html},
}

@misc{ledig_photo-realistic_2017,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = may,
	year = {2017},
	note = {arXiv:1609.04802 [cs, stat]},
	keywords = {Statistics - Machine Learning, GAN, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/EP5RYI8Q/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/KGA35TZH/1609.html:text/html},
}

@article{bruce_systematic_2016,
	title = {A systematic review of the psychosocial correlates of intuitive eating among adult women},
	volume = {96},
	issn = {01956663},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0195666315300635},
	doi = {10.1016/j.appet.2015.10.012},
	language = {en},
	urldate = {2022-10-12},
	journal = {Appetite},
	author = {Bruce, Lauren J. and Ricciardelli, Lina A.},
	month = jan,
	year = {2016},
	pages = {454--472},
}

@article{shah_soft_2021,
	title = {A soft robot that adapts to environments through shape change},
	volume = {3},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-020-00263-1},
	doi = {10.1038/s42256-020-00263-1},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {Nature Machine Intelligence},
	author = {Shah, Dylan S. and Powers, Joshua P. and Tilton, Liana G. and Kriegman, Sam and Bongard, Josh and Kramer-Bottiglio, Rebecca},
	month = jan,
	year = {2021},
	pages = {51--59},
	file = {Submitted Version:/Users/racicaleksa/Zotero/storage/8KGME5VV/Shah et al. - 2021 - A soft robot that adapts to environments through s.pdf:application/pdf},
}

@article{shah_soft_2021-1,
	title = {A soft robot that adapts to environments through shape change},
	volume = {3},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-020-00263-1},
	doi = {10.1038/s42256-020-00263-1},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {Nature Machine Intelligence},
	author = {Shah, Dylan S. and Powers, Joshua P. and Tilton, Liana G. and Kriegman, Sam and Bongard, Josh and Kramer-Bottiglio, Rebecca},
	month = jan,
	year = {2021},
	pages = {51--59},
	file = {Submitted Version:/Users/racicaleksa/Zotero/storage/FPBY2KQN/Shah et al. - 2021 - A soft robot that adapts to environments through s.pdf:application/pdf},
}

@article{shah_soft_2021-2,
	title = {A soft robot that adapts to environments through shape change},
	volume = {3},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-020-00263-1},
	doi = {10.1038/s42256-020-00263-1},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {Nature Machine Intelligence},
	author = {Shah, Dylan S. and Powers, Joshua P. and Tilton, Liana G. and Kriegman, Sam and Bongard, Josh and Kramer-Bottiglio, Rebecca},
	month = jan,
	year = {2021},
	pages = {51--59},
	file = {Submitted Version:/Users/racicaleksa/Zotero/storage/QZIAQV5B/Shah et al. - 2021 - A soft robot that adapts to environments through s.pdf:application/pdf},
}

@article{shah_soft_2021-3,
	title = {A soft robot that adapts to environments through shape change},
	volume = {3},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00263-1},
	doi = {10.1038/s42256-020-00263-1},
	abstract = {Many organisms, including various species of spiders and caterpillars, change their shape to switch gaits and adapt to different environments. Recent technological advances, ranging from stretchable circuits to highly deformable soft robots, have begun to make shape-changing robots a possibility. However, it is currently unclear how and when shape change should occur, and what capabilities could be gained, leading to a wide range of unsolved design and control problems. To begin addressing these questions, here we simulate, design and build a soft robot that utilizes shape change to achieve locomotion over both a flat and inclined surface. Modelling this robot in simulation, we explore its capabilities in two environments and demonstrate the automated discovery of environment-specific shapes and gaits that successfully transfer to the physical hardware. We found that the shape-changing robot traverses these environments better than an equivalent but non-morphing robot, in simulation and reality.},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {Nature Machine Intelligence},
	author = {Shah, Dylan S. and Powers, Joshua P. and Tilton, Liana G. and Kriegman, Sam and Bongard, Josh and Kramer-Bottiglio, Rebecca},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Mathematics and computing},
	pages = {51--59},
	file = {Submitted Version:/Users/racicaleksa/Zotero/storage/4V4J9GKS/Shah et al. - 2021 - A soft robot that adapts to environments through s.pdf:application/pdf},
}

@article{shah_soft_2021-4,
	title = {A soft robot that adapts to environments through shape change},
	volume = {3},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00263-1},
	doi = {10.1038/s42256-020-00263-1},
	abstract = {Many organisms, including various species of spiders and caterpillars, change their shape to switch gaits and adapt to different environments. Recent technological advances, ranging from stretchable circuits to highly deformable soft robots, have begun to make shape-changing robots a possibility. However, it is currently unclear how and when shape change should occur, and what capabilities could be gained, leading to a wide range of unsolved design and control problems. To begin addressing these questions, here we simulate, design and build a soft robot that utilizes shape change to achieve locomotion over both a flat and inclined surface. Modelling this robot in simulation, we explore its capabilities in two environments and demonstrate the automated discovery of environment-specific shapes and gaits that successfully transfer to the physical hardware. We found that the shape-changing robot traverses these environments better than an equivalent but non-morphing robot, in simulation and reality.},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {Nature Machine Intelligence},
	author = {Shah, Dylan S. and Powers, Joshua P. and Tilton, Liana G. and Kriegman, Sam and Bongard, Josh and Kramer-Bottiglio, Rebecca},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Mathematics and computing},
	pages = {51--59},
	file = {Submitted Version:/Users/racicaleksa/Zotero/storage/FUVUF9LJ/Shah et al. - 2021 - A soft robot that adapts to environments through s.pdf:application/pdf},
}

@misc{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/2104.07636},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = jun,
	year = {2021},
	note = {arXiv:2104.07636 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/TYFFT2ZR/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/FFISLZRW/2104.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:/Users/racicaleksa/Zotero/storage/UE7V7WM8/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@article{farsiu_fast_2004,
	title = {Fast and {Robust} {Multiframe} {Super} {Resolution}},
	volume = {13},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/document/1331445/},
	doi = {10.1109/TIP.2004.834669},
	abstract = {Super-resolution reconstruction produces one or a set of high-resolution images from a set of low-resolution images. In the last two decades, a variety of super-resolution methods have been proposed. These methods are usually very sensitive to their assumed model of data and noise, which limits their utility. This paper reviews some of these methods and addresses their shortcomings. We propose an alternate approach using 1 norm minimization and robust regularization based on a bilateral prior to deal with different data and noise models. This computationally inexpensive method is robust to errors in motion and blur estimation and results in images with sharp edges. Simulation results conﬁrm the effectiveness of our method and demonstrate its superiority to other super-resolution methods.},
	language = {en},
	number = {10},
	urldate = {2022-10-14},
	journal = {IEEE Transactions on Image Processing},
	author = {Farsiu, S. and Robinson, M.D. and Elad, M. and Milanfar, P.},
	month = oct,
	year = {2004},
	pages = {1327--1344},
	file = {Farsiu et al. - 2004 - Fast and Robust Multiframe Super Resolution.pdf:/Users/racicaleksa/Zotero/storage/F74KBZM4/Farsiu et al. - 2004 - Fast and Robust Multiframe Super Resolution.pdf:application/pdf},
}

@misc{qin_scene_2022,
	title = {Scene {Text} {Image} {Super}-{Resolution} via {Content} {Perceptual} {Loss} and {Criss}-{Cross} {Transformer} {Blocks}},
	url = {http://arxiv.org/abs/2210.06924},
	doi = {10.48550/arXiv.2210.06924},
	abstract = {Text image super-resolution is a unique and important task to enhance readability of text images to humans. It is widely used as pre-processing in scene text recognition. However, due to the complex degradation in natural scenes, recovering high-resolution texts from the low-resolution inputs is ambiguous and challenging. Existing methods mainly leverage deep neural networks trained with pixel-wise losses designed for natural image reconstruction, which ignore the unique character characteristics of texts. A few works proposed content-based losses. However, they only focus on text recognizers' accuracy, while the reconstructed images may still be ambiguous to humans. Further, they often have weak generalizability to handle cross languages. To this end, we present TATSR, a Text-Aware Text Super-Resolution framework, which effectively learns the unique text characteristics using Criss-Cross Transformer Blocks (CCTBs) and a novel Content Perceptual (CP) Loss. The CCTB extracts vertical and horizontal content information from text images by two orthogonal transformers, respectively. The CP Loss supervises the text reconstruction with content semantics by multi-scale text recognition features, which effectively incorporates content awareness into the framework. Extensive experiments on various language datasets demonstrate that TATSR outperforms state-of-the-art methods in terms of both recognition accuracy and human perception.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Qin, Rui and Wang, Bin and Tai, Yu-Wing},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06924 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, SUPER RESOLUTION},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/DLLYDZYA/Qin et al. - 2022 - Scene Text Image Super-Resolution via Content Perc.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/EAE9S28Z/2210.html:text/html},
}

@misc{bourcier_evaluating_2022,
	title = {Evaluating the {Label} {Efficiency} of {Contrastive} {Self}-{Supervised} {Learning} for {Multi}-{Resolution} {Satellite} {Imagery}},
	url = {http://arxiv.org/abs/2210.06786},
	doi = {10.48550/arXiv.2210.06786},
	abstract = {The application of deep neural networks to remote sensing imagery is often constrained by the lack of ground-truth annotations. Adressing this issue requires models that generalize efficiently from limited amounts of labeled data, allowing us to tackle a wider range of Earth observation tasks. Another challenge in this domain is developing algorithms that operate at variable spatial resolutions, e.g., for the problem of classifying land use at different scales. Recently, self-supervised learning has been applied in the remote sensing domain to exploit readily-available unlabeled data, and was shown to reduce or even close the gap with supervised learning. In this paper, we study self-supervised visual representation learning through the lens of label efficiency, for the task of land use classification on multi-resolution/multi-scale satellite images. We benchmark two contrastive self-supervised methods adapted from Momentum Contrast (MoCo) and provide evidence that these methods can be perform effectively given little downstream supervision, where randomly initialized networks fail to generalize. Moreover, they outperform out-of-domain pretraining alternatives. We use the large-scale fMoW dataset to pretrain and evaluate the networks, and validate our observations with transfer to the RESISC45 dataset.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {BOURCIER, Jules and Dashyan, Gohar and Chanussot, Jocelyn and Alahari, Karteek},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06786 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/386GUC2D/BOURCIER et al. - 2022 - Evaluating the Label Efficiency of Contrastive Sel.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/25DAHAB3/2210.html:text/html},
}

@misc{zhou_efficient_2022,
	title = {Efficient {Image} {Super}-{Resolution} using {Vast}-{Receptive}-{Field} {Attention}},
	url = {http://arxiv.org/abs/2210.05960},
	doi = {10.48550/arXiv.2210.05960},
	abstract = {The attention mechanism plays a pivotal role in designing advanced super-resolution (SR) networks. In this work, we design an efficient SR network by improving the attention mechanism. We start from a simple pixel attention module and gradually modify it to achieve better super-resolution performance with reduced parameters. The specific approaches include: (1) increasing the receptive field of the attention branch, (2) replacing large dense convolution kernels with depth-wise separable convolutions, and (3) introducing pixel normalization. These approaches paint a clear evolutionary roadmap for the design of attention mechanisms. Based on these observations, we propose VapSR, the VAst-receptive-field Pixel attention network. Experiments demonstrate the superior performance of VapSR. VapSR outperforms the present lightweight networks with even fewer parameters. And the light version of VapSR can use only 21.68\% and 28.18\% parameters of IMDB and RFDN to achieve similar performances to those networks. The code and models are available at https://github.com/zhoumumu/VapSR.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Zhou, Lin and Cai, Haoming and Gu, Jinjin and Li, Zheyuan and Liu, Yingqi and Chen, Xiangyu and Qiao, Yu and Dong, Chao},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05960 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/CZ89UUFF/Zhou et al. - 2022 - Efficient Image Super-Resolution using Vast-Recept.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/I7AZA6KZ/2210.html:text/html},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99\% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1.},
	language = {en},
	number = {2},
	urldate = {2022-10-16},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	month = sep,
	year = {2013},
	pages = {154--171},
	file = {Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:/Users/racicaleksa/Zotero/storage/XP47M827/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
}

@misc{barrett_measuring_2018,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://arxiv.org/abs/1807.04225},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Barrett, David G. T. and Hill, Felix and Santoro, Adam and Morcos, Ari S. and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {arXiv:1807.04225 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, main},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/PVET6W7I/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/HJ48HQBW/1807.html:text/html},
}

@misc{santoro_simple_2017,
	title = {A simple neural network module for relational reasoning},
	url = {http://arxiv.org/abs/1706.01427},
	abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
	month = jun,
	year = {2017},
	note = {arXiv:1706.01427 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, RNN},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/QZ6ZGHME/Santoro et al. - 2017 - A simple neural network module for relational reas.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/6XTS6U9Q/1706.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-08-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/racicaleksa/Zotero/storage/8P75G9IM/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{lecun_deep_2015-1,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-08-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {CNN},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/racicaleksa/Zotero/storage/ECC92MKF/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{hill_learning_2019,
	title = {{LEARNING} {TO} {MAKE} {ANALOGIES} {BY} {CONTRASTING} {ABSTRACT} {RELATIONAL} {STRUCTURE}},
	language = {en},
	author = {Hill, Felix and Santoro, Adam and Barrett, David G T and Morcos, Ari and Lillicrap, Tim},
	year = {2019},
	file = {Hill et al. - 2019 - LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT.pdf:/Users/racicaleksa/Zotero/storage/NGVE8EAC/Hill et al. - 2019 - LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT.pdf:application/pdf},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {PGM} {Dataset}},
	url = {https://paperswithcode.com/dataset/pgm},
	abstract = {PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.},
	language = {en},
	urldate = {2023-08-29},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/B25NUQSZ/pgm.html:text/html},
}

@misc{barrett_measuring_2018-1,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://arxiv.org/abs/1807.04225},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Barrett, David G. T. and Hill, Felix and Santoro, Adam and Morcos, Ari S. and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {arXiv:1807.04225 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/RAZM48E4/1807.html:text/html;Full Text PDF:/Users/racicaleksa/Zotero/storage/EG6KA88P/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks.pdf:application/pdf},
}

@misc{dong_scattering_2023,
	title = {The {Scattering} {Compositional} {Learner}: {Discovering} {Objects}, {Attributes}, {Relationships} in {Analogical} {Reasoning}},
	copyright = {MIT},
	shorttitle = {The {Scattering} {Compositional} {Learner}},
	url = {https://github.com/dhh1995/SCL},
	abstract = {PyTorch implementation for The Scattering Compositional Learner (SCL)},
	urldate = {2023-09-05},
	author = {Dong, Honghua},
	month = aug,
	year = {2023},
	note = {original-date: 2020-07-10T00:32:20Z},
	keywords = {visualization},
}

@misc{zheng_abstract_2023,
	title = {Abstract {Reasoning} with {Distracting} {Features}},
	url = {https://github.com/zkcys001/distracting_feature},
	abstract = {This is a PyTorch implement of ''Abstract Reasoning with Distracting Features'' that appears in the NeurIPS 2019.},
	urldate = {2023-09-05},
	author = {Zheng, Kecheng},
	month = aug,
	year = {2023},
	note = {original-date: 2019-09-08T01:22:43Z},
	keywords = {Modifikacija na org model},
}

@misc{zhang_copinet_2023,
	title = {{CoPINet}},
	copyright = {GPL-3.0},
	url = {https://github.com/WellyZhang/CoPINet},
	abstract = {Learning Perceptual Inference by Contrasting},
	urldate = {2023-09-05},
	author = {Zhang, Chi},
	month = mar,
	year = {2023},
	note = {original-date: 2019-11-26T01:45:59Z},
	keywords = {BoljiRezultati},
}

@article{zhang_learning_nodate,
	title = {Learning {Perceptual} {Inference} by {Contrasting}},
	abstract = {Thinking in pictures,” [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a signiﬁcant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artiﬁcial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain speciﬁc tasks. However, current AI’s ability in “thinking in pictures” is still far lacking behind. In this work, we study how to improve machines’ reasoning ability on one challenging task of this kind: Raven’s Progressive Matrices (RPM). Speciﬁcally, we borrow the very idea of “contrast effects” from the ﬁeld of psychology, cognition, and education to design and train a permutationinvariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatialtemporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.},
	language = {en},
	author = {Zhang, Chi and Jia, Baoxiong and Gao, Feng and Zhu, Yixin and Lu, Hongjing and Zhu, Song-Chun},
	file = {Zhang et al. - Learning Perceptual Inference by Contrasting.pdf:/Users/racicaleksa/Zotero/storage/QXDHZT9A/Zhang et al. - Learning Perceptual Inference by Contrasting.pdf:application/pdf},
}

@misc{noauthor_wrenreadmemd_nodate,
	title = {{WReN}/{README}.md at master · {Fen9}/{WReN}},
	url = {https://github.com/Fen9/WReN/blob/master/README.md},
	language = {en},
	urldate = {2023-09-08},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/QJEGZS5P/README.html:text/html},
}

@misc{noauthor_procedurally_2023,
	title = {Procedurally {Generated} {Matrices} ({PGM}) data},
	copyright = {Apache-2.0},
	url = {https://github.com/google-deepmind/abstract-reasoning-matrices},
	abstract = {Progressive matrices dataset, as described in: Measuring abstract reasoning in neural networks (Barrett*, Hill*, Santoro*, Morcos, Lillicrap), ICML2018},
	urldate = {2023-09-08},
	publisher = {Google DeepMind},
	month = jul,
	year = {2023},
	note = {original-date: 2018-06-07T12:40:21Z},
}

@misc{noauthor_authentication_nodate,
	title = {Authentication with {AWS} {Cognito}},
	url = {http://topstonesoftware.com/publications/authentication_with_aws_cognito.html},
	urldate = {2023-09-11},
	file = {Authentication with AWS Cognito:/Users/racicaleksa/Zotero/storage/N772AEWE/authentication_with_aws_cognito.html:text/html},
}

@misc{noauthor_creating_nodate,
	title = {Creating a {Cognito} {User} {Pool}},
	url = {http://topstonesoftware.com/publications/creating_a_cognito_user_pool.html},
	urldate = {2023-09-11},
	file = {Creating a Cognito User Pool:/Users/racicaleksa/Zotero/storage/A3KMCLXQ/creating_a_cognito_user_pool.html:text/html},
}

@misc{security_in_action_101_how_2022,
	title = {How to integrate {AWS} {Cognito} with {Google} {Social} login?},
	url = {https://www.youtube.com/watch?v=7r0eBNBNEZ8},
	abstract = {Setup Google as a Social Identity Provider in AWS Cognito.
\#openid \#identity \#iam \#security \#sso \#aws \#amazonwebservices \#cognito \#google \#awscognito \#amazoncognito \#sociallogin},
	urldate = {2023-09-11},
	author = {{Security in Action 101}},
	month = sep,
	year = {2022},
}

@misc{karthik_how_2023,
	title = {How to integrate {AWS} {Cognito} with {Google} {Social} login?},
	url = {https://awskarthik82.medium.com/how-to-integrate-aws-cognito-with-google-social-login-fd379ff644cc},
	abstract = {Introduction},
	language = {en},
	urldate = {2023-09-11},
	journal = {Medium},
	author = {karthik},
	month = jul,
	year = {2023},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/8FB8W9UM/how-to-integrate-aws-cognito-with-google-social-login-fd379ff644cc.html:text/html},
}

@misc{gupta_spring_2023,
	title = {Spring {Boot} {Role}-{Based} {Authentication} with {AWS} {Cognito}},
	url = {https://howtodoinjava.com/spring-security/spring-boot-role-based-authentication-with-aws-cognito/},
	abstract = {Learned to configure the user pool in the AWS Cognito, create the app client, integrate the client with our Spring boot application using Spring Security.},
	language = {en-US},
	urldate = {2023-09-11},
	journal = {HowToDoInJava},
	author = {Gupta, Lokesh},
	month = jul,
	year = {2023},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/LFZ3EVZ9/spring-boot-role-based-authentication-with-aws-cognito.html:text/html},
}

@misc{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1402.1128},
	doi = {10.48550/arXiv.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	month = feb,
	year = {2014},
	note = {arXiv:1402.1128 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/YIGJU86H/Sak et al. - 2014 - Long Short-Term Memory Based Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/8U3RWXES/1402.html:text/html},
}

@misc{statquest_with_josh_starmer_long_2022,
	title = {Long {Short}-{Term} {Memory} ({LSTM}), {Clearly} {Explained}},
	url = {https://www.youtube.com/watch?v=YCzL96nL7j0},
	abstract = {Basic recurrent neural networks are great, because they can handle different amounts of sequential data, but even relatively small sequences of data can make them difficult to train. This is where Long Short-Term Memory (LSTM) saves the day. Long Short-Term Memory is a type of recurrent neural network that can handle much larger sequences of data without those pesky exploding/vanishing gradient problems that plague basic recurrent neural networks.

Spanish
Este video ha sido doblado al español con voz artificial con https://aloud.area120.google.com para aumentar la accesibilidad. Puede cambiar el idioma de la pista de audio en el menú Configuración.

Portuguese
Este vídeo foi dublado para o português usando uma voz artificial via https://aloud.area120.google.com para melhorar sua acessibilidade. Você pode alterar o idioma do áudio no menu Configurações.

For a complete index of all the StatQuest videos, check out...
https://app.learney.me/maps/StatQuest
...or...
https://statquest.org/video-index/

If you'd like to support StatQuest, please consider...
Patreon: https://www.patreon.com/statquest
...or...
YouTube Membership:    / @statquest  

...a cool StatQuest t-shirt or sweatshirt: 
https://shop.spreadshirt.com/statques...

...buying one or two of my songs (or go large and get a whole album!)
https://joshuastarmer.bandcamp.com/

...or just donating to StatQuest!
https://www.paypal.me/statquest

Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter:
https://twitter.com/joshuastarmer

0:00 Awesome song, introduction and main ideas
4:19 The sigmoid and tanh activation functions
5:58 LSTM Stage 1: The percent to remember
9:25 LSTM Stage 2: Update the long-term memory
12:42 LSTM Stage 3:Update the short-term memory
14:33 LSTM in action with real data

\#StatQuest \#LSTM \#Dubbedwithaloud},
	urldate = {2023-09-11},
	author = {{StatQuest with Josh Starmer}},
	month = nov,
	year = {2022},
}

@misc{oktadev_id_2023,
	title = {{ID} {Tokens} {VS} {Access} {Tokens}: {What}'s the {Difference}?},
	shorttitle = {{ID} {Tokens} {VS} {Access} {Tokens}},
	url = {https://www.youtube.com/watch?v=vVM1Tpu9QB4},
	abstract = {ID Tokens vs Access Tokens. What are they and when do you use them? How do they differ? Where do they come from? We'll briefly cover OAuth 2.0 and OpenID Connect and the difference between Authentication and Authorization. 

Grab the FREE Cheat Sheet from the Auth0 by Okta blog post - 
https://auth0.com/blog/id-token-acces...

Sign up for our monthly newsletter! https://a0.to/zeroindex

\#authentication \#developer \#tokens \#login 
 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Learn with Auth0 by Okta
Try for free - https://a0.to/auth0
The Auth0 by Okta blog - https://a0.to/blog
Ask questions on the Community Forum - https://a0.to/community \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ 
Follow Us on Social 
Twitter - https://twitter.com/oktadev
LinkedIn - https://www.linkedin.com/company/oktadev},
	urldate = {2023-09-13},
	author = {{OktaDev}},
	month = jan,
	year = {2023},
}

@misc{noauthor_configuring_nodate,
	title = {Configuring a user pool app client - {Amazon} {Cognito}},
	url = {https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-client-apps.html},
	urldate = {2023-09-13},
	file = {Configuring a user pool app client - Amazon Cognito:/Users/racicaleksa/Zotero/storage/3IFZELSL/user-pool-settings-client-apps.html:text/html},
}

@misc{noauthor_verifying_nodate,
	title = {Verifying a {JSON} {Web} {Token} - {Amazon} {Cognito}},
	url = {https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html},
	urldate = {2023-09-13},
	file = {Verifying a JSON Web Token - Amazon Cognito:/Users/racicaleksa/Zotero/storage/ELXVLAMC/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html:text/html},
}

@misc{noauthor_implementing_2023,
	title = {Implementing {Magic} {Links} with {Amazon} {Cognito}: {A} {Step}-by-{Step} {Guide}},
	shorttitle = {Implementing {Magic} {Links} with {Amazon} {Cognito}},
	url = {https://theburningmonk.com/2023/03/implementing-magic-links-with-amazon-cognito-a-step-by-step-guide/},
	abstract = {Learn to build production-ready serverless applications on AWS},
	language = {en-US},
	urldate = {2023-09-14},
	journal = {theburningmonk.com},
	month = mar,
	year = {2023},
	note = {Section: AWS},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/LPPQJP37/implementing-magic-links-with-amazon-cognito-a-step-by-step-guide.html:text/html},
}

@misc{noauthor_implementing_2019,
	title = {Implementing passwordless email authentication with {Amazon} {Cognito} {\textbar} {Front}-{End} {Web} \& {Mobile}},
	url = {https://aws.amazon.com/blogs/mobile/implementing-passwordless-email-authentication-with-amazon-cognito/},
	language = {en-US},
	urldate = {2023-09-14},
	month = jan,
	year = {2019},
	note = {Section: Amazon Cognito},
}

@misc{noauthor_passwordless_2023,
	title = {Passwordless {Authentication} made easy with {Cognito}: a step-by-step guide},
	shorttitle = {Passwordless {Authentication} made easy with {Cognito}},
	url = {https://theburningmonk.com/2023/03/passwordless-authentication-made-easy-with-cognito-a-step-by-step-guide/},
	abstract = {Learn to build production-ready serverless applications on AWS},
	language = {en-US},
	urldate = {2023-09-14},
	journal = {theburningmonk.com},
	month = mar,
	year = {2023},
	note = {Section: AWS},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/5F6E3A4U/passwordless-authentication-made-easy-with-cognito-a-step-by-step-guide.html:text/html},
}

@misc{hoshen_iq_2017,
	title = {{IQ} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.01692},
	abstract = {IQ tests are an accepted method for assessing human intelligence. The tests consist of several parts that must be solved under a time constraint. Of all the tested abilities, pattern recognition has been found to have the highest correlation with general intelligence. This is primarily because pattern recognition is the ability to find order in a noisy environment, a necessary skill for intelligent agents. In this paper, we propose a convolutional neural network (CNN) model for solving geometric pattern recognition problems. The CNN receives as input multiple ordered input images and outputs the next image according to the pattern. Our CNN is able to solve problems involving rotation, reflection, color, size and shape patterns and score within the top 5\% of human performance.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Hoshen, Dokhyam and Werman, Michael},
	month = sep,
	year = {2017},
	note = {arXiv:1710.01692 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/NJ58BT8R/1710.html:text/html;Full Text PDF:/Users/racicaleksa/Zotero/storage/MWSJEILN/Hoshen and Werman - 2017 - IQ of Neural Networks.pdf:application/pdf},
}
