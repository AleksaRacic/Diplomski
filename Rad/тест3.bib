
@misc{barrett_measuring_2018,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://arxiv.org/abs/1807.04225},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Barrett, David G. T. and Hill, Felix and Santoro, Adam and Morcos, Ari S. and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {arXiv:1807.04225 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, main},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/PVET6W7I/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/HJ48HQBW/1807.html:text/html},
}

@misc{santoro_simple_2017,
	title = {A simple neural network module for relational reasoning},
	url = {http://arxiv.org/abs/1706.01427},
	abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
	month = jun,
	year = {2017},
	note = {arXiv:1706.01427 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, RNN},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/QZ6ZGHME/Santoro et al. - 2017 - A simple neural network module for relational reas.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/6XTS6U9Q/1706.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-08-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {CNN},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/racicaleksa/Zotero/storage/ECC92MKF/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{hill_learning_2019,
	title = {{LEARNING} {TO} {MAKE} {ANALOGIES} {BY} {CONTRASTING} {ABSTRACT} {RELATIONAL} {STRUCTURE}},
	language = {en},
	author = {Hill, Felix and Santoro, Adam and Barrett, David G T and Morcos, Ari and Lillicrap, Tim},
	year = {2019},
	file = {Hill et al. - 2019 - LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT.pdf:/Users/racicaleksa/Zotero/storage/NGVE8EAC/Hill et al. - 2019 - LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT.pdf:application/pdf},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {PGM} {Dataset}},
	url = {https://paperswithcode.com/dataset/pgm},
	abstract = {PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.},
	language = {en},
	urldate = {2023-08-29},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/B25NUQSZ/pgm.html:text/html},
}

@misc{dong_scattering_2023,
	title = {The {Scattering} {Compositional} {Learner}: {Discovering} {Objects}, {Attributes}, {Relationships} in {Analogical} {Reasoning}},
	copyright = {MIT},
	shorttitle = {The {Scattering} {Compositional} {Learner}},
	url = {https://github.com/dhh1995/SCL},
	abstract = {PyTorch implementation for The Scattering Compositional Learner (SCL)},
	urldate = {2023-09-05},
	author = {Dong, Honghua},
	month = aug,
	year = {2023},
	note = {original-date: 2020-07-10T00:32:20Z},
	keywords = {visualization},
}

@misc{zheng_abstract_2023,
	title = {Abstract {Reasoning} with {Distracting} {Features}},
	url = {https://github.com/zkcys001/distracting_feature},
	abstract = {This is a PyTorch implement of ''Abstract Reasoning with Distracting Features'' that appears in the NeurIPS 2019.},
	urldate = {2023-09-05},
	author = {Zheng, Kecheng},
	month = aug,
	year = {2023},
	note = {original-date: 2019-09-08T01:22:43Z},
	keywords = {Modifikacija na org model},
}

@misc{zhang_copinet_2023,
	title = {{CoPINet}},
	copyright = {GPL-3.0},
	url = {https://github.com/WellyZhang/CoPINet},
	abstract = {Learning Perceptual Inference by Contrasting},
	urldate = {2023-09-05},
	author = {Zhang, Chi},
	month = mar,
	year = {2023},
	note = {original-date: 2019-11-26T01:45:59Z},
	keywords = {BoljiRezultati},
}

@article{zhang_learning_nodate,
	title = {Learning {Perceptual} {Inference} by {Contrasting}},
	abstract = {Thinking in pictures,” [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a signiﬁcant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artiﬁcial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain speciﬁc tasks. However, current AI’s ability in “thinking in pictures” is still far lacking behind. In this work, we study how to improve machines’ reasoning ability on one challenging task of this kind: Raven’s Progressive Matrices (RPM). Speciﬁcally, we borrow the very idea of “contrast effects” from the ﬁeld of psychology, cognition, and education to design and train a permutationinvariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatialtemporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.},
	language = {en},
	author = {Zhang, Chi and Jia, Baoxiong and Gao, Feng and Zhu, Yixin and Lu, Hongjing and Zhu, Song-Chun},
	file = {Zhang et al. - Learning Perceptual Inference by Contrasting.pdf:/Users/racicaleksa/Zotero/storage/QXDHZT9A/Zhang et al. - Learning Perceptual Inference by Contrasting.pdf:application/pdf},
}

@misc{noauthor_wrenreadmemd_nodate,
	title = {{WReN}/{README}.md at master · {Fen9}/{WReN}},
	url = {https://github.com/Fen9/WReN/blob/master/README.md},
	language = {en},
	urldate = {2023-09-08},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/QJEGZS5P/README.html:text/html},
}

@misc{noauthor_procedurally_2023,
	title = {Procedurally {Generated} {Matrices} ({PGM}) data},
	copyright = {Apache-2.0},
	url = {https://github.com/google-deepmind/abstract-reasoning-matrices},
	abstract = {Progressive matrices dataset, as described in: Measuring abstract reasoning in neural networks (Barrett*, Hill*, Santoro*, Morcos, Lillicrap), ICML2018},
	urldate = {2023-09-08},
	publisher = {Google DeepMind},
	month = jul,
	year = {2023},
	note = {original-date: 2018-06-07T12:40:21Z},
}

@misc{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1402.1128},
	doi = {10.48550/arXiv.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	month = feb,
	year = {2014},
	note = {arXiv:1402.1128 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/YIGJU86H/Sak et al. - 2014 - Long Short-Term Memory Based Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/8U3RWXES/1402.html:text/html},
}

@misc{statquest_with_josh_starmer_long_2022,
	title = {Long {Short}-{Term} {Memory} ({LSTM}), {Clearly} {Explained}},
	url = {https://www.youtube.com/watch?v=YCzL96nL7j0},
	abstract = {Basic recurrent neural networks are great, because they can handle different amounts of sequential data, but even relatively small sequences of data can make them difficult to train. This is where Long Short-Term Memory (LSTM) saves the day. Long Short-Term Memory is a type of recurrent neural network that can handle much larger sequences of data without those pesky exploding/vanishing gradient problems that plague basic recurrent neural networks.

Spanish
Este video ha sido doblado al español con voz artificial con https://aloud.area120.google.com para aumentar la accesibilidad. Puede cambiar el idioma de la pista de audio en el menú Configuración.

Portuguese
Este vídeo foi dublado para o português usando uma voz artificial via https://aloud.area120.google.com para melhorar sua acessibilidade. Você pode alterar o idioma do áudio no menu Configurações.

For a complete index of all the StatQuest videos, check out...
https://app.learney.me/maps/StatQuest
...or...
https://statquest.org/video-index/

If you'd like to support StatQuest, please consider...
Patreon: https://www.patreon.com/statquest
...or...
YouTube Membership:    / @statquest  

...a cool StatQuest t-shirt or sweatshirt: 
https://shop.spreadshirt.com/statques...

...buying one or two of my songs (or go large and get a whole album!)
https://joshuastarmer.bandcamp.com/

...or just donating to StatQuest!
https://www.paypal.me/statquest

Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter:
https://twitter.com/joshuastarmer

0:00 Awesome song, introduction and main ideas
4:19 The sigmoid and tanh activation functions
5:58 LSTM Stage 1: The percent to remember
9:25 LSTM Stage 2: Update the long-term memory
12:42 LSTM Stage 3:Update the short-term memory
14:33 LSTM in action with real data

\#StatQuest \#LSTM \#Dubbedwithaloud},
	urldate = {2023-09-11},
	author = {{StatQuest with Josh Starmer}},
	month = nov,
	year = {2022},
}

@misc{hoshen_iq_2017,
	title = {{IQ} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.01692},
	abstract = {IQ tests are an accepted method for assessing human intelligence. The tests consist of several parts that must be solved under a time constraint. Of all the tested abilities, pattern recognition has been found to have the highest correlation with general intelligence. This is primarily because pattern recognition is the ability to find order in a noisy environment, a necessary skill for intelligent agents. In this paper, we propose a convolutional neural network (CNN) model for solving geometric pattern recognition problems. The CNN receives as input multiple ordered input images and outputs the next image according to the pattern. Our CNN is able to solve problems involving rotation, reflection, color, size and shape patterns and score within the top 5\% of human performance.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Hoshen, Dokhyam and Werman, Michael},
	month = sep,
	year = {2017},
	note = {arXiv:1710.01692 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/NJ58BT8R/1710.html:text/html;Full Text PDF:/Users/racicaleksa/Zotero/storage/MWSJEILN/Hoshen and Werman - 2017 - IQ of Neural Networks.pdf:application/pdf},
}
