@article{raven1938raven,
  title={Raven standard progressive matrices},
  author={Raven, John C},
  journal={Journal of Cognition and Development},
  year={1938}
}

@article{te2001practice,
  title={Practice and coaching on IQ tests: Quite a lot of g},
  author={Te Nijenhuis, Jan and Voskuijl, Olga F and Schijve, Natasja B},
  journal={International Journal of Selection and Assessment},
  volume={9},
  number={4},
  pages={302--308},
  year={2001},
  publisher={Wiley Online Library}
}

@misc{santoro_simple_2017,
	title = {A simple neural network module for relational reasoning},
	url = {http://arxiv.org/abs/1706.01427},
	abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
	month = jun,
	year = {2017},
	note = {arXiv:1706.01427 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, RNN},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/QZ6ZGHME/Santoro et al. - 2017 - A simple neural network module for relational reas.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/6XTS6U9Q/1706.html:text/html},
}

@misc{barrett_measuring_2018,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://arxiv.org/abs/1807.04225},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Barrett, David G. T. and Hill, Felix and Santoro, Adam and Morcos, Ari S. and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {arXiv:1807.04225 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, main},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/PVET6W7I/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/HJ48HQBW/1807.html:text/html},
}

@web{noauthor_procedurally_2023,
	title = {Procedurally {Generated} {Matrices} ({PGM}) data},
	copyright = {Apache-2.0},
	url = {https://github.com/google-deepmind/abstract-reasoning-matrices},
	abstract = {Progressive matrices dataset, as described in: Measuring abstract reasoning in neural networks (Barrett*, Hill*, Santoro*, Morcos, Lillicrap), ICML2018},
	urldate = {2023-09-08},
	publisher = {Google DeepMind},
	month = jul,
	year = {2023},
	note = {original-date: 2018-06-07T12:40:21Z},
}


@phdthesis{ioannou_structural_2017,
	abstract = {Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. Despite breakthroughs in training deep networks, there remains a lack of understanding of both the optimization and structure of deep networks. The approach advocated by many researchers in the field has been to train monolithic networks with excess complexity, and strong regularization --- an approach that leaves much to desire in efficiency. Instead we propose that carefully designing networks in consideration of our prior knowledge of the task and learned representation can improve the memory and compute efficiency of state-of-the art networks, and even improve generalization --- what we propose to denote as structural priors. We present two such novel structural priors for convolutional neural networks, and evaluate them in state-of-the-art image classification CNN architectures. The first of these methods proposes to exploit our knowledge of the low-rank nature of most filters learned for natural images by structuring a deep network to learn a collection of mostly small, low-rank, filters. The second addresses the filter/channel extents of convolutional filters, by learning filters with limited channel extents. The size of these channel-wise basis filters increases with the depth of the model, giving a novel sparse connection structure that resembles a tree root. Both methods are found to improve the generalization of these architectures while also decreasing the size and increasing the efficiency of their training and test-time computation. Finally, we present work towards conditional computation in deep neural networks, moving towards a method of automatically learning structural priors in deep networks. We propose a new discriminative learning model, conditional networks, that jointly exploit the accurate representation learning capabilities of deep neural networks with the efficient conditional computation of decision trees. Conditional networks yield smaller models, and offer test-time flexibility in the trade-off of computation vs. accuracy.},
	author = {Ioannou, Yani},
	doi = {10.17863/CAM.26357},
	file = {Full Text PDF:/Users/racicaleksa/Zotero/storage/LE7BK2Y2/Ioannou - 2017 - Structural Priors in Deep Neural Networks.pdf:application/pdf},
	month = sep,
	title = {Structural {Priors} in {Deep} {Neural} {Networks}},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.17863/CAM.26357}}


@article{anwar_learned_2017,
	abstract = {Ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as Human Computer Interaction (HCI); demographic based classification; biometric based recognition; security and defense to name a few. In this paper we present a novel approach for extracting ethnicity from the facial images. The proposed method makes use of a pre trained Convolutional Neural Network (CNN) to extract the features and then Support Vector Machine (SVM) with linear kernel is used as a classifier. This technique uses translational invariant hierarchical features learned by the network, in contrast to previous works, which use hand crafted features such as Local Binary Pattern (LBP); Gabor etc. Thorough experiments are presented on ten different facial databases which strongly suggest that our approach is robust to different expressions and illuminations conditions. Here we consider ethnicity classification as a three class problem including Asian, African-American and Caucasian. Average classification accuracy over all databases is 98.28\%, 99.66\% and 99.05\% for Asian, African-American and Caucasian respectively.},
	author = {Anwar, Inzamam and Ul Islam, Naeem},
	doi = {10.1515/cait-2017-0036},
	file = {Full Text:/Users/racicaleksa/Zotero/storage/6MAS4QC2/Anwar and Ul Islam - 2017 - Learned Features are Better for Ethnicity Classifi.pdf:application/pdf},
	journal = {Cybernetics and Information Technologies},
	month = sep,
	title = {Learned {Features} are {Better} for {Ethnicity} {Classification}},
	volume = {17},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1515/cait-2017-0036}}


@misc{noauthor_artificial_2023,
	abstract = {Artificial neural networks (ANNs, also shortened to neural networks (NNs) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.

Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.},
	copyright = {Creative Commons Attribution-ShareAlike License},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/3TA8ZASC/Artificial_neural_network.html:text/html},
	journal = {Wikipedia},
	language = {en},
	month = sep,
	note = {Page Version ID: 1175471829},
	title = {Artificial neural network},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1175471829},
	urldate = {2023-09-15},
	year = {2023},
	bdsk-url-1 = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1175471829}}




@phdthesis{s_mohamed_detection_2017,
	abstract = {The problem of developing an autonomous forklift that is able to pick-up and place pallets is not new. The same is true for pallet detection and localization, which pose interesting perception challenges due to their sparse structure. Many approaches have been presented for solving the problems of extraction, segmentation, and estimation of the pallet based on vision and Laser Rangefinder (LRF) systems. Here, the focus of attention is on the possibility of solving the problem by using a 2D LRF. On the other hand, machine learning has become a major field of research in order to handle more and more complex detection and recognition problems. The aim of this thesis is to develop a new and robust system for identifying, localizing, and tracking the pallets based on machine learning approaches, especially Convolutional Neural Network (CNN). The proposed system is mainly composed of two main components: Faster Region-based Convolutional Network (Faster R-CNN) detector and CNN classifier for detecting and recognizing the pallets, and a simple Kalman filter for tracking and increasing the confidence of the presence of the pallet. For fine-tuning the proposed CNNs, the system is tested systematically on real-world data containing 340 labelled object examples. Finally, performance is evaluated given the average accuracy over k-fold cross-validation. The computational complexity of the proposed system is also evaluated. Finally, the experimental results are presented, using MATLAB and ROS, verifying the feasibility and good performance of the proposed system. The best performance is achieved by our proposed CNN with an average accuracy of 99.58\% for a k-fold of 10. Regarding the tracking task, the experiments are performed while the robot was moving towards the pallet. Due to availability, the experiments are carried out by considering only one pallet, and consequently to check
the robustness of our algorithm, an artificial data are generated by considering one more pallet in the environment. It is observed that our system is able to recognize and track the positive tracks (pallets) among other negatives tracks with high confidence scores.},
	author = {S. Mohamed, Ihab},
	doi = {10.13140/RG.2.2.30795.69926},
	file = {Full Text PDF:/Users/racicaleksa/Zotero/storage/CAJ7WT8W/S. Mohamed - 2017 - Detection and Tracking of Pallets using a Laser Ra.pdf:application/pdf},
	month = sep,
	title = {Detection and {Tracking} of {Pallets} using a {Laser} {Rangefinder} and {Machine} {Learning} {Techniques}},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.13140/RG.2.2.30795.69926}}


@article{phung_high-accuracy_2019,
	abstract = {Research on clouds has an enormous influence on sky sciences and related applications, and cloud classification plays an essential role in it. Much research has been conducted which includes both traditional machine learning approaches and deep learning approaches. Compared with traditional machine learning approaches, deep learning approaches achieved better results. However, most deep learning models need large data to train due to the large number of parameters. Therefore, they cannot get high accuracy in case of small datasets. In this paper, we propose a complete solution for high accuracy of classification of cloud image patches on small datasets. Firstly, we designed a suitable convolutional neural network (CNN) model for small datasets. Secondly, we applied regularization techniques to increase generalization and avoid overfitting of the model. Finally, we introduce a model average ensemble to reduce the variance of prediction and increase the classification accuracy. We experiment the proposed solution on the Singapore whole-sky imaging categories (SWIMCAT) dataset, which demonstrates perfect classification accuracy for most classes and confirms the robustness of the proposed model.},
	author = {{Phung} and {Rhee}},
	doi = {10.3390/app9214500},
	file = {Full Text:/Users/racicaleksa/Zotero/storage/FYMKJ5DN/Phung and Rhee - 2019 - A High-Accuracy Model Average Ensemble of Convolut.pdf:application/pdf},
	journal = {Applied Sciences},
	month = oct,
	pages = {4500},
	title = {A {High}-{Accuracy} {Model} {Average} {Ensemble} of {Convolutional} {Neural} {Networks} for {Classification} of {Cloud} {Image} {Patches} on {Small} {Datasets}},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/app9214500}}


@article{husein_day-ahead_2019,
	abstract = {In microgrids, forecasting solar power output is crucial for optimizing operation and reducing the impact of uncertainty. To forecast solar power output, it is essential to forecast solar irradiance, which typically requires historical solar irradiance data. These data are often unavailable for residential and commercial microgrids that incorporate solar photovoltaic. In this study, we propose an hourly day-ahead solar irradiance forecasting model that does not depend on the historical solar irradiance data; it uses only widely available weather data, namely, dry-bulb temperature, dew-point temperature, and relative humidity. The model was developed using a deep, long short-term memory recurrent neural network (LSTM-RNN). We compare this approach with a feedforward neural network (FFNN), which is a method with a proven record of accomplishment in solar irradiance forecasting. To provide a comprehensive evaluation of this approach, we performed six experiments using measurement data from weather stations in Germany, U.S.A, Switzerland, and South Korea, which all have distinct climate types. Experiment results show that the proposed approach is more accurate than FFNN, and achieves the accuracy of up to 60.31 W/m2 in terms of root-mean-square error (RMSE). Moreover, compared with the persistence model, the proposed model achieves average forecast skill of 50.90\% and up to 68.89\% in some datasets. In addition, to demonstrate the effect of using a particular forecasting model on the microgrid operation optimization, we simulate a one-year operation of a commercial building microgrid. Results show that the proposed approach is more accurate, and leads to a 2\% rise in annual energy savings compared with FFNN.},
	author = {Husein, Munir and Chung, Il-Yop},
	doi = {10.3390/en12101856},
	file = {Full Text:/Users/racicaleksa/Zotero/storage/6PGBF9Q8/Husein and Chung - 2019 - Day-Ahead Solar Irradiance Forecasting for Microgr.pdf:application/pdf},
	journal = {Energies},
	month = may,
	pages = {1856},
	shorttitle = {Day-{Ahead} {Solar} {Irradiance} {Forecasting} for {Microgrids} {Using} a {Long} {Short}-{Term} {Memory} {Recurrent} {Neural} {Network}},
	title = {Day-{Ahead} {Solar} {Irradiance} {Forecasting} for {Microgrids} {Using} a {Long} {Short}-{Term} {Memory} {Recurrent} {Neural} {Network}: {A} {Deep} {Learning} {Approach}},
	volume = {12},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/en12101856}}


@article{qiu_forecasting_2020,
	abstract = {The stock market is known for its extreme complexity and volatility, and people are always looking for an accurate and effective way to guide stock trading. Long short-term memory (LSTM) neural networks are developed by recurrent neural networks (RNN) and have significant application value in many fields. In addition, LSTM avoids long-term dependence issues due to its unique storage unit structure, and it helps predict financial time series. Based on LSTM and an attention mechanism, a wavelet transform is used to denoise historical stock data, extract and train its features, and establish the prediction model of a stock price. We compared the results with the other three models, including the LSTM model, the LSTM model with wavelet denoising and the gated recurrent unit(GRU) neural network model on S\&P 500, DJIA, HSI datasets. Results from experiments on the S\&P 500 and DJIA datasets show that the coefficient of determination of the attention-based LSTM model is both higher than 0.94, and the mean square error of our model is both lower than 0.05.},
	author = {Qiu, Jiayu and Wang, Bin and Zhou, Changjun},
	doi = {10.1371/journal.pone.0227222},
	file = {Full Text:/Users/racicaleksa/Zotero/storage/WFLYZVTR/Qiu et al. - 2020 - Forecasting stock prices with long-short term memo.pdf:application/pdf},
	journal = {PLOS ONE},
	month = jan,
	pages = {e0227222},
	title = {Forecasting stock prices with long-short term memory neural network based on attention mechanism},
	volume = {15},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0227222}}



@article{h_sultan_multi-classification_2019,
	abstract = {Brain tumor classification is a crucial task to evaluate the tumors and make a treatment decision according to their classes. There are many imaging techniques used to detect brain tumors. However, MRI is commonly used due to its superior image quality and the fact of relying on no ionizing radiation. Deep learning (DL) is a subfield of machine learning and recently showed a remarkable performance especially in classification and segmentation problems. In this paper, a DL model based on a convolutional neural network is proposed to classify different brain tumor types using two publicly available datasets. The former one classifies tumors into (meningioma, glioma, and pituitary tumor). The other one differentiates between three glioma grades (Grade II, Grade III, and Grade IV). Datasets include 233 and 73 patients with a total of 3064 and 516 images on T1-weighted contrast-enhanced images for the first and second datasets respectively. The proposed network structure achieves a significant performance with a best overall accuracy of 96.13\% and 98.7\% respectively for the two studies. Results indicate the ability of the model for brain tumor multi-classification purposes.},
	author = {H. Sultan, Hossam and Salem, Nancy and Al-Atabany, Walid},
	doi = {10.1109/ACCESS.2019.2919122},
	journal = {IEEE Access},
	month = may,
	pages = {1--1},
	title = {Multi-{Classification} of {Brain} {Tumor} {Images} {Using} {Deep} {Neural} {Network}},
	volume = {PP},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2019.2919122}}


@book{van1995python, 
  title={Python reference manual}, 
  author={Van Rossum, Guido and Drake Jr, Fred L}, 
  year={1995}, 
  publisher={Centrum voor Wiskunde en Informatica Amsterdam} 
}

@inproceedings{torch, 
  title = {Torch7: A Matlab-like Environment for Machine Learning}, 
  author = {R. Collobert and K. Kavukcuoglu and C. Farabet}, 
  booktitle = {BigLearn, NIPS Workshop}, 
  year = {2011} 
}


@misc{hoshen_iq_2017,
	abstract = {IQ tests are an accepted method for assessing human intelligence. The tests consist of several parts that must be solved under a time constraint. Of all the tested abilities, pattern recognition has been found to have the highest correlation with general intelligence. This is primarily because pattern recognition is the ability to find order in a noisy environment, a necessary skill for intelligent agents. In this paper, we propose a convolutional neural network (CNN) model for solving geometric pattern recognition problems. The CNN receives as input multiple ordered input images and outputs the next image according to the pattern. Our CNN is able to solve problems involving rotation, reflection, color, size and shape patterns and score within the top 5\% of human performance.},
	author = {Hoshen, Dokhyam and Werman, Michael},
	file = {arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/D9X75YX9/1710.html:text/html;Full Text PDF:/Users/racicaleksa/Zotero/storage/Q2SDQZGL/Hoshen and Werman - 2017 - IQ of Neural Networks.pdf:application/pdf},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	month = sep,
	note = {arXiv:1710.01692 [cs]},
	publisher = {arXiv},
	title = {{IQ} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.01692},
	urldate = {2023-09-16},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1710.01692}}




@misc{kingma_adam_2017,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	doi = {10.48550/arXiv.1412.6980},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/R8QW5FK5/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/WFDFIGU7/1412.html:text/html},
	keywords = {Computer Science - Machine Learning},
	month = jan,
	note = {arXiv:1412.6980 [cs]},
	publisher = {arXiv},
	shorttitle = {Adam},
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	urldate = {2023-09-17},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1412.6980},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1412.6980}}
